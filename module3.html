<!DOCTYPE HTML>
<html>
<head>
  <title>Deciphering Big Data — Portfolio</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">
  <div id="wrapper">

    <!-- Header -->
    <header id="header">
      <a href="index.html" class="logo"><strong>Kalifa Burton</strong> <span>Portfolio</span></a>
      <nav><a href="#menu">Menu</a></nav>
    </header>

    <!-- Menu -->
    <nav id="menu">
      <ul class="links">
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="module3.html">Deciphering Big Data</a></li>
        <li><a href="scraper.html">Scraper</a></li>
      </ul>
    </nav>

    <!-- Main -->
    <section id="main" class="wrapper">
      <div class="inner">

        <header class="major">
          <h2>Module 3 — Deciphering Big Data</h2>
          <p>Module summary: Exploring diverse data types, sources, and methods of collection, while developing skills in data wrangling</p> 
          <p>including collection and publishing.  Learning how to improve data quality by cleaning, normalising, and transforming raw data.</p>
        </header>

        <!-- Learning outcomes -->
        <section>
          <h3>Learning outcomes</h3>
          <ul>
            <li>Identify and manage challenges, security issues and risks, limitations and opportunities in data wrangling</li>
            <li>Critically analyse data wrangling issues, determine appropriate methods, tools and techniques to solve them.</li>
            <li>Design, construct and evaluate solutions for processing datasets and solving complex problems in various environments using relevant programming paradigms.</li>
            <li>Develop and implement skills required to be effective member of development team in a virtual professional environment, adopting realistic perspectives on team roles.</li>
          </ul>
        </section>

        <!-- Artefacts -->
        <section>
          <h3>Artefacts created during the module</h3>

          <article id="iot-discussion-artefact">
  <h4>Internet of Things (IoT) Discussion (Artefact)</h4>
  <p>
    A critical evaluation of the rationale behind the Internet of Things (IoT), discussing opportunities, limitations, and risks of large-scale data collection, with attention to data cleaning, edge/fog processing, and ethical governance. This artefact includes an initial post and a summary post that briefly acknowledges peer perspectives (Sonia and Dean) while centring my own argument.
  </p>

  <!-- Optional quick nav within the article -->
  <p><em>Jump to:</em>
    <a href="#iot-initial-post">Initial post</a> ·
    <a href="#iot-summary-post">Summary post</a> ·
    <a href="#iot-reflection">Reflection</a> ·
    <a href="#iot-references">References</a>
  </p>

  <details id="iot-initial-post" open>
    <summary><strong>Initial Post</strong></summary>
    <p>The Internet of Things (IoT) represents a paradigm shift in data generation and usage, underlining modern day big data ecosystems.</p>
    <p>The idea behind it lies in enabling seamless data collection from interconnected devices that allow for optimisation of decision-making,</p>
    <p>efficiency and innovation across sectors (Sardar et al., 2024).  Embedding sensors and smart technologies within physical infrastructure,</p>
    <p> IoT helps towards giving live insights which aid in improvement of healthcare monitoring, environmental management, as well as automation.</p>

    <p>However, the large-scale collection of heterogeneous data introduces substantial challenges in terms of data quality, privacy, and governance.</p>
    <p> Huxley (2020) emphasises that the reliability of any data-driven system depends on rigorous data cleaning and validation processes, which</p>
    <p>is a critical yet often overlooked stage that affects the accuracy and ethical use of IoT datasets. Furthermore, scholars such as</p>
    <p>Alaba et al. (2017) highlight security vulnerabilities in IoT networks, while Atlam et al. (2020) argue that unregulated data</p>
    <p>aggregation risks surveillance, bias, and inequity.</p>
          
    <p>Thus, while IoT offers transformative opportunities, its rationale must be critically evaluated within frameworks that ensure transparency,</p>
    <p>accountability, and ethical data stewardship. Sustainable IoT deployment requires balancing innovation with trust, privacy protection,</p>
    <p> and methodological rigour in data management.</p>
  </details>

  <details id="iot-summary-post">
    <summary><strong>Summary Post</strong></summary>
    <p><h3>Summary Post </h3>
          <p>The Internet of Things (IoT) represents a critical intersection between connectivity, data science, and ethical governance.</p>
          <p>In my initial discussion, I argued that while IoT offers transformative potential across healthcare, industry, and environmental systems,</p>
          <p>its success depends on how effectively the resulting data is cleaned, validated, and ethically managed.  Huxley (2020) highlights that</p>
          <p>robust data cleaning is fundamental to ensuring analytical accuracy, a principle that underpins the reliability of IoT-derived insights.</p>

          <p>>Building on this, Sardar et al.(2024) emphasise that scalable big data architectures must balance performance with privacy and accountability</p>
          <p>which is an essential consideration given the projected growth of IoT to billions of interconnected devices (Gunjal et al., 2024).  Sonia’s</p>
          <p>observation regarding the use of fog computing aligns with this, as decentralised processing can reduce latency and enhance efficiency.</p>
          <p>However, as I discussed, distributed systems also expand the potential for data inconsistencies and privacy breaches if governance</p>
          <p>mechanisms are insufficient (Atlam et al., 2020).</p>
          <p>Dean’s reflections on the ethical dimensions of IoT further reinforce the need for transparent data management practices.</p>
          <p>These perspectives collectively affirm that the rationale for IoT extends beyond technological innovation: it is about cultivating </p>
          <p>trust through rigorous data quality assurance, ethical design, and resilience against misuse.</p>
          
          <p>In conclusion, effective IoT implementation requires synergy between technological scalability and human-centred responsibility,</p>
          <p> in turn ensuring that the data connecting our world remains not only vast but also verifiable, secure, and ethically sound.</p>
  </details>

  <details id="iot-reflection">
    <summary><strong>Reflection</strong></summary>
    <p>This discussion strengthened my ability to link architectural scalability with data ethics. Engaging with my peers discussions</p>
    <p>helped me refine arguments around edge/fog trade-offs, security, and the centrality of data cleaning to valid analytics.</p>
    <p>I will apply these insights by prioritising governance controls and validation pipelines in future IoT-related projects.</p>
  </details>

  <details id="iot-references">
    <summary><strong>References</strong></summary>
    <ul>
       <li>Alaba, F.A., Othman, M., Hashem, I.A.T. and Alotaibi, F. (2017) ‘Internet of Things security: A survey’, <em>Journal of Network and Computer Applications</em>, 88, pp. 10–28. doi:10.1016/j.jnca.2017.04.002.</li>
       <li>Atlam, H.F. and Wills, G.B. (2020) ‘IoT security, privacy, safety and ethics’, in Farsi, M., Daneshkhah, A., Hosseinian-Far, A. and Jahankhani, H. (eds.) <em>Digital Twin Technologies and Smart Cities</em>. Cham: Springer, pp. 1–27. doi:10.1007/978-3-030-18732-3.</li>
       <li>Gunjal, P.R., Jondhale, S.R., Lloret Mauri, J. and Agrawal, K. (2024) <em>Internet of Things: Theory to Practice</em>. Abingdon: CRC Press.</li>
       <li>Huxley, J. (2020) ‘Data Cleaning’, in <em>SAGE Research Methods Foundations</em>. London: SAGE. doi:10.4135/9781526421036842861.</li>
       <li>Sardar, T.H. and Pandey, B.K. (eds.) (2024) <em>Big Data Computing: Advances in Technologies, Methodologies, and Applications</em>. Boca Raton: CRC Press. doi:10.1201/9781032634050.</li>
    </ul>
  </details>
</article>


          <article>
            <h4>Data Scientist Job Scraper (Artefact)</h4>
            <p>Short description: a Python script that extracts job titles containing the keyword <strong>Data Scientist</strong> from a saved job-search HTML page and saves the results to JSON (and optionally XML).</p>
            <p><strong>Files:</strong></p>
            <ul>
              <li><a href="simplyhired_scraper2.py" target="_blank">Python script — simplyhired_scraper2.py</a></li>
              <li><a href="sample_simplyhired.html" target="_blank">Saved page — sample_simplyhired.html</a></li>
              <li><a href="jobs_from_html.json" target="_blank">Output JSON — jobs_from_html.json</a></li>
            </ul>

            <p><strong>Notes on limitations / feedback:</strong> The script uses a saved HTML page to avoid site blocking; company names may not always be captured from the static save. (Include any tutor or peer feedback you received here.)</p>
          </article>

          <article>
  <h4>Internet of Things (IoT) Discussion (Artefact)</h4>
  <p>A critical evaluation of the rationale behind IoT, discussing opportunities, limitations, and risks of</p>
  <p>large-scale data collection, with attention to data cleaning, edge/fog processing, and ethical governance.</p>
  <p><strong>Contents:</strong> Initial Post and Summary Post.</p>
  <p><a href="artefacts/iot-discussion.html" class="button">View IoT Discussion Artefact</a></p>
</article>


          <!-- Add additional artefacts by copying the <article> block -->
          <article>
  <h4>Artefact: Data Management Pipeline Test</h4>

  <p>
    This artefact comes from my <em>Data Management Pipeline Test</em>, which evaluated my understanding of
    Python programming best practices and how they apply to building robust data workflows.
    The test required matching Python concepts, libraries, and development principles with their correct purposes or use cases.
  </p>

  <p>
    The screenshots below demonstrate my successful completion of these exercises, highlighting my knowledge of
    Python syntax standards, repository organization, documentation, and effective use of libraries and helper
    functions within a data pipeline context.
  </p>

  <h5>Figures</h5>

  <ul>
    <li><strong>Figure 1.</strong> Matching Python concepts and their purposes <br>
      <em>Demonstrating understanding of CSV handling, string formatting, and datetime conversion.</em></li>
    <li><strong>Figure 2.</strong> Python best practices for clean and efficient development <br>
      <em>Covering syntax, naming conventions, testing, repository organisation, and version control.</em></li>
    <li><strong>Figure 3.</strong> Imports, helper functions, and library usage <br>
      <em>Emphasising efficient coding practices and reusability in Python projects.</em></li>
  </ul>

  <!-- Image gallery -->
  <div class="gallery">
    <figure>
      <img src="images/Matching python concepts.png" alt="Matching Python concepts exercise">
      <figcaption>Figure 1. Matching Python Concepts</figcaption>
    </figure>
    <figure>
      <img src="images/Python practice with a description.png" alt="Python best practices part 1">
      <figcaption>Figure 2. Python Best Practices (Set 1)</figcaption>
    </figure>
    <figure>
      <img src="images/Python practice with a description 2.png" alt="Python best practices part 2">
      <figcaption>Figure 3. Python Best Practices (Set 2)</figcaption>
    </figure>
    <figure>
      <img src="images/Answer corrections.png" alt="Answer corrections summary">
      <figcaption>Figure 4. Answer Corrections</figcaption>
    </figure>
  </div>

  <hr>

  <h5>Learning Reflection</h5>
  <p>
    Completing this Data Management Pipeline Test helped me deepen my understanding of how Python best practices
    underpin effective data pipeline development. I learned how to apply the PEP-8 standard for clean syntax,
    use clear and meaningful variable names, and implement version control to manage code evolution
    in collaborative environments.
  </p>

  <p>
    I also gained a stronger awareness of the importance of modular code, through the use of helper functions and
    external libraries, which improve efficiency and maintainability in large-scale data processing systems. These principles are
    directly relevant to building data management pipelines, where clarity, structure, and testing are essential
    for ensuring data integrity, reproducibility, and scalability.
  </p>
</article>


        </section>

        <!-- Reflective piece -->
        <section>
          <h3>Reflection</h3>
          <p>What I learned and how: In creating the scraper I gained practical experience with HTTP requests, HTML parsing using BeautifulSoup, handling site restrictions, and structuring output data for analysis (JSON/XML). I improved debugging strategies and learned to document limitations and ethical considerations for web scraping.</p>
        </section>

        <!-- Meeting notes -->
        <section>
          <h3>Meeting notes</h3>
          <p>Team meeting (06/09/2025): Shared progress, reviewed code, and agreed next steps.</p>
          <p>To add more: upload meeting notes files (PDF/MD) to repo and link them here.</p>
        </section>

        <!-- Professional Skills Matrix & Action Plan -->
        <section>
          <h3>Professional Skills Matrix & Action Plan</h3>

          <table style="width:100%; border-collapse: collapse;">
            <thead>
              <tr style="background:#f0f0f0;">
                <th style="border: 1px solid #ddd; padding:8px;">Skill</th>
                <th style="border: 1px solid #ddd; padding:8px;">Current level</th>
                <th style="border: 1px solid #ddd; padding:8px;">Evidence (artefact)</th>
                <th style="border: 1px solid #ddd; padding:8px;">Action plan</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="border: 1px solid #ddd; padding:8px;">Python (web scraping)</td>
                <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
                <td style="border: 1px solid #ddd; padding:8px;">simplyhired_scraper2.py</td>
                <td style="border: 1px solid #ddd; padding:8px;">Refine selectors, learn requests sessions and polite scraping, explore APIs</td>
              </tr>
              <tr>
                <td style="border: 1px solid #ddd; padding:8px;">Data wrangling</td>
                <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
                <td style="border: 1px solid #ddd; padding:8px;">jobs_from_html.json</td>
                <td style="border: 1px solid #ddd; padding:8px;">Create a cleaned CSV, visualise results</td>
              </tr>
            <tbody>
  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Python (web scraping)</td>
    <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
    <td style="border: 1px solid #ddd; padding:8px;">simplyhired_scraper2.py</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Refine selectors, use sessions for polite scraping, explore APIs for dynamic content retrieval.
    </td>
  </tr>

  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Data wrangling</td>
    <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
    <td style="border: 1px solid #ddd; padding:8px;">jobs_from_html.json</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Clean extracted data, convert to structured formats (CSV/JSON), and visualise results.
    </td>
  </tr>

  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Python best practices</td>
    <td style="border: 1px solid #ddd; padding:8px;">Competent</td>
    <td style="border: 1px solid #ddd; padding:8px;">Data Management Pipeline Test (Figures 1–4)</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Implement consistent PEP-8 standards across scripts; expand use of helper functions, 
      add unit testing to validate pipeline reliability.
    </td>
  </tr>

  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Version control (Git/GitHub)</td>
    <td style="border: 1px solid #ddd; padding:8px;">Competent</td>
    <td style="border: 1px solid #ddd; padding:8px;">E-Portfolio repository commits and artefact updates</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Create feature branches for new artefacts; document commit messages more descriptively; 
      use pull requests for peer code review.
    </td>
  </tr>

  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Data pipeline design</td>
    <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
    <td style="border: 1px solid #ddd; padding:8px;">Data Management Pipeline Test</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Design and implement a simple ETL workflow using Python; integrate CSV/JSON ingestion, 
      cleaning, transformation, and output validation.
    </td>
  </tr>

  <tr>
    <td style="border: 1px solid #ddd; padding:8px;">Documentation and communication</td>
    <td style="border: 1px solid #ddd; padding:8px;">Developing</td>
    <td style="border: 1px solid #ddd; padding:8px;">Module 3 E-portfolio write-ups and artefact reflections</td>
    <td style="border: 1px solid #ddd; padding:8px;">
      Continue writing concise, reflective summaries for each artefact; 
      improve Markdown documentation and inline code comments.
    </td>
  </tr>
</tbody>

          </table>

        </section>

      </div>
    </section>

    <!-- Footer -->
    <footer id="footer">
      <div class="inner">
        <p>&copy; Kalifa</p>
      </div>
    </footer>

  </div>

  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/main.js"></script>
</body>
</html>
